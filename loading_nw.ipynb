{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DOING THE SAME AS FOR THE MME_DATASET, BUT THEN FOR ONLY COUNTRIES *NOT* IN THE WESTERN LIST'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DOING THE SAME AS FOR THE MME_DATASET, BUT THEN FOR ONLY COUNTRIES *NOT* IN THE WESTERN LIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a dataframe with only the UserCountry3 column from the 71M SharedResponses.csv file\n",
    "reader_country = pd.read_csv('SharedResponses.csv', usecols=['UserCountry3', 'ResponseID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries considered 'Western' from the UN\n",
    "\n",
    "countries = [\"AND\",  \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DNK\", \"DEU\", \"ESP\", \"FIN\",  \"FRA\",  \"GBR\",  \"GRC\",  \"IRL\", \n",
    "            \"ISR\", \"ISL\", \"ITA\", \"LIE\", \"LUX\", \"MLT\", \"MCO\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SMR\", \"SWE\", \"TUR\", \"USA\" ]\n",
    "\n",
    "# Andorra, Australia, Austria, Belgium, Canada, Switzerland, Denmark, Germany, Spain, Finland, France, United Kingdom, Greece, Ireland, \n",
    "# Israel, Iceland, Italy, Liechtenstein, Luxembourg, Malta, Monaco, Netherlands, Norway, New Zealand, Portugal, San Marino, Sweden, Turkey, USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where UserCountryID is NOT the list of countries that are considered Western\n",
    "\n",
    "non_western = reader_country[~reader_country['UserCountry3'].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21837017, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_western.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep complete scenarios of 2\n",
    "\n",
    "# Count how many times each ResponseID appears\n",
    "response_counts = non_western['ResponseID'].value_counts()\n",
    "\n",
    "# Get the ResponseIDs that appear exactly twice\n",
    "ids_to_keep = response_counts[response_counts == 2].index\n",
    "\n",
    "# Filter the rows where ResponseID is in the list of ids_to_keep\n",
    "complete_responseid_country = non_western[non_western['ResponseID'].isin(ids_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(complete_responseid_country.value_counts().min())\n",
    "print(complete_responseid_country.value_counts().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21084418, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_responseid_country.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644868\n"
     ]
    }
   ],
   "source": [
    "# checking the NAN's in UserCountry3 in the complete_responseid_country dataset and removing them\n",
    "print(complete_responseid_country['UserCountry3'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(20439550, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing the NAN's\n",
    "complete_responseid_country = complete_responseid_country.dropna(subset=['UserCountry3'])\n",
    "print(complete_responseid_country['UserCountry3'].isna().sum())\n",
    "print(complete_responseid_country.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on 'ResponseID' and keep the first occurrence\n",
    "complete_responseid_country = complete_responseid_country.drop_duplicates(subset=['ResponseID'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_subset = complete_responseid_country.sample(n=3500000)  # want 7M rows (with accounting for NAn's and deleting rows with 'random' (around 10%)), so 17M / 2 = 8.5M ResponseID's necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseID      3500000\n",
       "UserCountry3        202\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if they are all unique\n",
    "\n",
    "country_subset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500000\n"
     ]
    }
   ],
   "source": [
    "# transform to a list to feed to the for loop that will extract all the corresponding columns from the SharedResponses.csv file\n",
    "\n",
    "country_sub_list = country_subset['ResponseID'].tolist()\n",
    "# check if it went okay\n",
    "print(len(country_sub_list))     # should be 8500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_country = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Finished processing chunk 1\n",
      "Processing chunk 2\n",
      "Finished processing chunk 2\n",
      "Processing chunk 3\n",
      "Finished processing chunk 3\n",
      "Processing chunk 4\n",
      "Finished processing chunk 4\n",
      "Processing chunk 5\n",
      "Finished processing chunk 5\n",
      "Processing chunk 6\n",
      "Finished processing chunk 6\n",
      "Processing chunk 7\n",
      "Finished processing chunk 7\n",
      "Processing chunk 8\n",
      "Finished processing chunk 8\n",
      "Processing chunk 9\n",
      "Finished processing chunk 9\n",
      "Processing chunk 10\n",
      "Finished processing chunk 10\n",
      "Processing chunk 11\n",
      "Finished processing chunk 11\n",
      "Processing chunk 12\n",
      "Finished processing chunk 12\n",
      "Processing chunk 13\n",
      "Finished processing chunk 13\n",
      "Processing chunk 14\n",
      "Finished processing chunk 14\n",
      "Processing chunk 15\n",
      "Finished processing chunk 15\n",
      "Processing chunk 16\n",
      "Finished processing chunk 16\n",
      "Processing chunk 17\n",
      "Finished processing chunk 17\n",
      "Processing chunk 18\n",
      "Finished processing chunk 18\n",
      "Processing chunk 19\n",
      "Finished processing chunk 19\n",
      "Processing chunk 20\n",
      "Finished processing chunk 20\n",
      "Processing chunk 21\n",
      "Finished processing chunk 21\n",
      "Processing chunk 22\n",
      "Finished processing chunk 22\n",
      "Processing chunk 23\n",
      "Finished processing chunk 23\n",
      "Processing chunk 24\n",
      "Finished processing chunk 24\n",
      "Processing chunk 25\n",
      "Finished processing chunk 25\n",
      "Processing chunk 26\n",
      "Finished processing chunk 26\n",
      "Processing chunk 27\n",
      "Finished processing chunk 27\n",
      "Processing chunk 28\n",
      "Finished processing chunk 28\n",
      "Processing chunk 29\n",
      "Finished processing chunk 29\n",
      "Processing chunk 30\n",
      "Finished processing chunk 30\n",
      "Processing chunk 31\n",
      "Finished processing chunk 31\n",
      "Processing chunk 32\n",
      "Finished processing chunk 32\n",
      "Processing chunk 33\n",
      "Finished processing chunk 33\n",
      "Processing chunk 34\n",
      "Finished processing chunk 34\n",
      "Processing chunk 35\n",
      "Finished processing chunk 35\n",
      "Processing chunk 36\n",
      "Finished processing chunk 36\n",
      "Processing chunk 37\n",
      "Finished processing chunk 37\n",
      "Processing chunk 38\n",
      "Finished processing chunk 38\n",
      "Processing chunk 39\n",
      "Finished processing chunk 39\n",
      "Processing chunk 40\n",
      "Finished processing chunk 40\n",
      "Processing chunk 41\n",
      "Finished processing chunk 41\n",
      "Processing chunk 42\n",
      "Finished processing chunk 42\n",
      "Processing chunk 43\n",
      "Finished processing chunk 43\n",
      "Processing chunk 44\n",
      "Finished processing chunk 44\n",
      "Processing chunk 45\n",
      "Finished processing chunk 45\n",
      "Processing chunk 46\n",
      "Finished processing chunk 46\n",
      "Processing chunk 47\n",
      "Finished processing chunk 47\n",
      "Processing chunk 48\n",
      "Finished processing chunk 48\n",
      "Processing chunk 49\n",
      "Finished processing chunk 49\n",
      "Processing chunk 50\n",
      "Finished processing chunk 50\n",
      "Processing chunk 51\n",
      "Finished processing chunk 51\n",
      "Processing chunk 52\n",
      "Finished processing chunk 52\n",
      "Processing chunk 53\n",
      "Finished processing chunk 53\n",
      "Processing chunk 54\n",
      "Finished processing chunk 54\n",
      "Processing chunk 55\n",
      "Finished processing chunk 55\n",
      "Processing chunk 56\n",
      "Finished processing chunk 56\n",
      "Processing chunk 57\n",
      "Finished processing chunk 57\n",
      "Processing chunk 58\n",
      "Finished processing chunk 58\n",
      "Processing chunk 59\n",
      "Finished processing chunk 59\n",
      "Processing chunk 60\n",
      "Finished processing chunk 60\n",
      "Processing chunk 61\n",
      "Finished processing chunk 61\n",
      "Processing chunk 62\n",
      "Finished processing chunk 62\n",
      "Processing chunk 63\n",
      "Finished processing chunk 63\n",
      "Processing chunk 64\n",
      "Finished processing chunk 64\n",
      "Processing chunk 65\n",
      "Finished processing chunk 65\n",
      "Processing chunk 66\n",
      "Finished processing chunk 66\n",
      "Processing chunk 67\n",
      "Finished processing chunk 67\n",
      "Processing chunk 68\n",
      "Finished processing chunk 68\n",
      "Processing chunk 69\n",
      "Finished processing chunk 69\n",
      "Processing chunk 70\n",
      "Finished processing chunk 70\n",
      "Processing chunk 71\n",
      "Finished processing chunk 71\n",
      "Processing chunk 72\n",
      "Finished processing chunk 72\n",
      "Processing chunk 73\n",
      "Finished processing chunk 73\n",
      "Processing chunk 74\n",
      "Finished processing chunk 74\n",
      "Processing chunk 75\n",
      "Finished processing chunk 75\n",
      "Processing chunk 76\n",
      "Finished processing chunk 76\n",
      "Processing chunk 77\n",
      "Finished processing chunk 77\n",
      "Processing chunk 78\n",
      "Finished processing chunk 78\n",
      "Processing chunk 79\n",
      "Finished processing chunk 79\n",
      "Processing chunk 80\n",
      "Finished processing chunk 80\n",
      "Processing chunk 81\n",
      "Finished processing chunk 81\n",
      "Processing chunk 82\n",
      "Finished processing chunk 82\n",
      "Processing chunk 83\n",
      "Finished processing chunk 83\n",
      "Processing chunk 84\n",
      "Finished processing chunk 84\n",
      "Processing chunk 85\n",
      "Finished processing chunk 85\n",
      "Processing chunk 86\n",
      "Finished processing chunk 86\n",
      "Processing chunk 87\n",
      "Finished processing chunk 87\n",
      "Processing chunk 88\n",
      "Finished processing chunk 88\n",
      "Processing chunk 89\n",
      "Finished processing chunk 89\n",
      "Processing chunk 90\n",
      "Finished processing chunk 90\n",
      "Processing chunk 91\n",
      "Finished processing chunk 91\n",
      "Processing chunk 92\n",
      "Finished processing chunk 92\n",
      "Processing chunk 93\n",
      "Finished processing chunk 93\n",
      "Processing chunk 94\n",
      "Finished processing chunk 94\n",
      "Processing chunk 95\n",
      "Finished processing chunk 95\n",
      "Processing chunk 96\n",
      "Finished processing chunk 96\n",
      "Processing chunk 97\n",
      "Finished processing chunk 97\n",
      "Processing chunk 98\n",
      "Finished processing chunk 98\n",
      "Processing chunk 99\n",
      "Finished processing chunk 99\n",
      "Processing chunk 100\n",
      "Finished processing chunk 100\n",
      "Processing chunk 101\n",
      "Finished processing chunk 101\n",
      "Processing chunk 102\n",
      "Finished processing chunk 102\n",
      "Processing chunk 103\n",
      "Finished processing chunk 103\n",
      "Processing chunk 104\n",
      "Finished processing chunk 104\n",
      "Processing chunk 105\n",
      "Finished processing chunk 105\n",
      "Processing chunk 106\n",
      "Finished processing chunk 106\n",
      "Processing chunk 107\n",
      "Finished processing chunk 107\n",
      "Processing chunk 108\n",
      "Finished processing chunk 108\n",
      "Processing chunk 109\n",
      "Finished processing chunk 109\n",
      "Processing chunk 110\n",
      "Finished processing chunk 110\n",
      "Processing chunk 111\n",
      "Finished processing chunk 111\n",
      "Processing chunk 112\n",
      "Finished processing chunk 112\n",
      "Processing chunk 113\n",
      "Finished processing chunk 113\n",
      "Processing chunk 114\n",
      "Finished processing chunk 114\n",
      "Processing chunk 115\n",
      "Finished processing chunk 115\n",
      "Processing chunk 116\n",
      "Finished processing chunk 116\n",
      "Processing chunk 117\n",
      "Finished processing chunk 117\n",
      "Processing chunk 118\n",
      "Finished processing chunk 118\n",
      "Processing chunk 119\n",
      "Finished processing chunk 119\n",
      "Processing chunk 120\n",
      "Finished processing chunk 120\n",
      "Processing chunk 121\n",
      "Finished processing chunk 121\n",
      "Processing chunk 122\n",
      "Finished processing chunk 122\n",
      "Processing chunk 123\n",
      "Finished processing chunk 123\n",
      "Processing chunk 124\n",
      "Finished processing chunk 124\n",
      "Processing chunk 125\n",
      "Finished processing chunk 125\n",
      "Processing chunk 126\n",
      "Finished processing chunk 126\n",
      "Processing chunk 127\n",
      "Finished processing chunk 127\n",
      "Processing chunk 128\n",
      "Finished processing chunk 128\n",
      "Processing chunk 129\n",
      "Finished processing chunk 129\n",
      "Processing chunk 130\n",
      "Finished processing chunk 130\n",
      "Processing chunk 131\n",
      "Finished processing chunk 131\n",
      "Processing chunk 132\n",
      "Finished processing chunk 132\n",
      "Processing chunk 133\n",
      "Finished processing chunk 133\n",
      "Processing chunk 134\n",
      "Finished processing chunk 134\n",
      "Processing chunk 135\n",
      "Finished processing chunk 135\n",
      "Processing chunk 136\n",
      "Finished processing chunk 136\n",
      "Processing chunk 137\n",
      "Finished processing chunk 137\n",
      "Processing chunk 138\n",
      "Finished processing chunk 138\n",
      "Processing chunk 139\n",
      "Finished processing chunk 139\n",
      "Processing chunk 140\n",
      "Finished processing chunk 140\n",
      "Processing chunk 141\n",
      "Finished processing chunk 141\n",
      "All chunks have been processed and combined.\n"
     ]
    }
   ],
   "source": [
    "# reading SharedResponses to extract the rows with ResponseID's in country_sub_list\n",
    "# this will result in a dataframe 'subset' that contains around 17 million rows (8,5 * 2)\n",
    "\n",
    "chunk_size = 500_000\n",
    "reader = pd.read_csv('SharedResponses.csv', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "for i, chunk in enumerate(reader):\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}\")\n",
    "\n",
    "    # Filter rows where ResponseID is in reader_subset\n",
    "    subset_chunk = chunk[chunk['ResponseID'].isin(country_sub_list)]\n",
    "\n",
    "    # Append filtered chunk to empty df\n",
    "    subset_country = pd.concat([subset_country, subset_chunk], ignore_index=True)\n",
    "\n",
    "    print(f\"Finished processing chunk {i+1}\")\n",
    "\n",
    "print(\"All chunks have been processed and combined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000000, 41)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_country.shape  # 7M rows, 41 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserCountry3\n",
      "False    7000000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking to see if the UserCountry3 column only contains countries not in the Western list\n",
    "print(subset_country['UserCountry3'].isin(countries).value_counts())\n",
    "\n",
    "# should be False for all rows (17M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_country_csv = subset_country.to_csv('subset_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esmku\\AppData\\Local\\Temp\\ipykernel_28492\\2704394548.py:2: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_country = pd.read_csv('subset_country.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_country = pd.read_csv('subset_country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScenarioTypeStrict\n",
       "Utilitarian      1257090\n",
       "Gender           1235454\n",
       "Age              1235056\n",
       "Species          1234144\n",
       "Fitness          1232254\n",
       "Random            615572\n",
       "Social Status     190430\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country['ScenarioTypeStrict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows with 'random' in the ScenarioTypeStrict column\n",
    "\n",
    "df_country = df_country[df_country['ScenarioTypeStrict'] != 'Random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScenarioTypeStrict\n",
       "Utilitarian      1257090\n",
       "Gender           1235454\n",
       "Age              1235056\n",
       "Species          1234144\n",
       "Fitness          1232254\n",
       "Social Status     190430\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country['ScenarioTypeStrict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttributeLevel\n",
       "More       628545\n",
       "Less       628545\n",
       "Male       617727\n",
       "Female     617727\n",
       "Pets       617071\n",
       "Hoomans    617071\n",
       "Old        596848\n",
       "Young      596848\n",
       "Fat        556024\n",
       "Fit        556024\n",
       "Rand       203272\n",
       "Low         74363\n",
       "High        74363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing all rows with 'random' in the ScenarioTypeStrict column, as LLM's don't have a random scenario\n",
    "df_country['AttributeLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df_country[df_country['AttributeLevel'] != 'Rand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttributeLevel\n",
       "More       628545\n",
       "Less       628545\n",
       "Male       617727\n",
       "Female     617727\n",
       "Pets       617071\n",
       "Hoomans    617071\n",
       "Old        596848\n",
       "Young      596848\n",
       "Fat        556024\n",
       "Fit        556024\n",
       "Low         74363\n",
       "High        74363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing all rows with 'Rand' in the AttributeLevel column, as LLM's don't have a random scenario\n",
    "df_country['AttributeLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6181156, 41)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseID                      0\n",
       "ExtendedSessionID               0\n",
       "UserID                        350\n",
       "ScenarioOrder                   0\n",
       "Intervention                    0\n",
       "PedPed                          0\n",
       "Barrier                         0\n",
       "CrossingSignal                  0\n",
       "AttributeLevel                  0\n",
       "ScenarioTypeStrict              0\n",
       "ScenarioType                    0\n",
       "DefaultChoice                   0\n",
       "NonDefaultChoice                0\n",
       "DefaultChoiceIsOmission         0\n",
       "NumberOfCharacters              0\n",
       "DiffNumberOFCharacters          0\n",
       "Saved                           0\n",
       "Template                   740348\n",
       "DescriptionShown           740348\n",
       "LeftHand                   740348\n",
       "UserCountry3                    0\n",
       "Man                             0\n",
       "Woman                           0\n",
       "Pregnant                        0\n",
       "Stroller                        0\n",
       "OldMan                          0\n",
       "OldWoman                        0\n",
       "Boy                             0\n",
       "Girl                            0\n",
       "Homeless                        0\n",
       "LargeWoman                      0\n",
       "LargeMan                        0\n",
       "Criminal                        0\n",
       "MaleExecutive                   0\n",
       "FemaleExecutive                 0\n",
       "FemaleAthlete                   0\n",
       "MaleAthlete                     0\n",
       "FemaleDoctor                    0\n",
       "MaleDoctor                      0\n",
       "Dog                             0\n",
       "Cat                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows with NaN's in the UserID column\n",
    "\n",
    "df_country = df_country.dropna(subset=['UserID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6180806, 41)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.shape # should be 6181156 - 350 = 6180806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total dataset has to be 5M rows.\n",
    "# 2% of that is LLM's, so 100.000 rows\n",
    "# the other 98% will be humans, so 4.900.000 rows\n",
    "\n",
    "# need to subset 4900000 rows from the 6180806 rows\n",
    "# need to delete 6180806 - 4900000 = 1280806 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3090403\n"
     ]
    }
   ],
   "source": [
    "# randomly delete 1280806 / 2 = 640.403 unique UserIDs (is 1280806 rows), to ensure 2% of the dataset is LLMs and 98% humans\n",
    "\n",
    "# Getting unique UserIDs\n",
    "Response_unique = df_country['ResponseID'].unique()\n",
    "print(len(Response_unique))  # should be 6180806/2 = 3090403\n",
    "\n",
    "# Selecting 559467 UserIDs from the unique set\n",
    "Response_delete = pd.Series(Response_unique).sample(n=640403, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_98 = df_country[~df_country['ResponseID'].isin(Response_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900000, 41)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of df_filtered\n",
    "\n",
    "df_country_98.shape   # should be 4.9M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>ExtendedSessionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>ScenarioOrder</th>\n",
       "      <th>Intervention</th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>AttributeLevel</th>\n",
       "      <th>ScenarioTypeStrict</th>\n",
       "      <th>...</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2224kBG72574tbZD3</td>\n",
       "      <td>737909459_839962439872333.0</td>\n",
       "      <td>8.399624e+14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Old</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2228EQY3XnmTh3dq8</td>\n",
       "      <td>-1700464608_947225429117679.0</td>\n",
       "      <td>9.472254e+14</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2229qWoicHtyXWHJd</td>\n",
       "      <td>596330438_8731233368451661.0</td>\n",
       "      <td>8.731233e+15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Young</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222HpiEf2LtAwEg62</td>\n",
       "      <td>-1232628507_1597557389</td>\n",
       "      <td>1.597557e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gender</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>222bh6ceSDJvxhaak</td>\n",
       "      <td>329642844_496228042967179.0</td>\n",
       "      <td>4.962280e+14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pets</td>\n",
       "      <td>Species</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ResponseID              ExtendedSessionID        UserID  \\\n",
       "0  2224kBG72574tbZD3    737909459_839962439872333.0  8.399624e+14   \n",
       "2  2228EQY3XnmTh3dq8  -1700464608_947225429117679.0  9.472254e+14   \n",
       "3  2229qWoicHtyXWHJd   596330438_8731233368451661.0  8.731233e+15   \n",
       "4  222HpiEf2LtAwEg62         -1232628507_1597557389  1.597557e+09   \n",
       "6  222bh6ceSDJvxhaak    329642844_496228042967179.0  4.962280e+14   \n",
       "\n",
       "   ScenarioOrder  Intervention  PedPed  Barrier  CrossingSignal  \\\n",
       "0             12             0       0        1               0   \n",
       "2              7             0       0        0               0   \n",
       "3              6             0       1        0               0   \n",
       "4              5             0       0        1               0   \n",
       "6              4             0       1        0               0   \n",
       "\n",
       "  AttributeLevel ScenarioTypeStrict  ... LargeMan Criminal MaleExecutive  \\\n",
       "0            Old                Age  ...      0.0      0.0           0.0   \n",
       "2           More        Utilitarian  ...      0.0      0.0           0.0   \n",
       "3          Young                Age  ...      0.0      0.0           0.0   \n",
       "4         Female             Gender  ...      0.0      0.0           0.0   \n",
       "6           Pets            Species  ...      0.0      0.0           0.0   \n",
       "\n",
       "   FemaleExecutive  FemaleAthlete  MaleAthlete  FemaleDoctor MaleDoctor  Dog  \\\n",
       "0              0.0            0.0          0.0           0.0        0.0  0.0   \n",
       "2              0.0            0.0          1.0           0.0        0.0  0.0   \n",
       "3              0.0            0.0          0.0           0.0        0.0  0.0   \n",
       "4              0.0            0.0          0.0           0.0        0.0  0.0   \n",
       "6              0.0            0.0          0.0           0.0        0.0  0.0   \n",
       "\n",
       "   Cat  \n",
       "0  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "6  2.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([0, 884580], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# check what the distribution of RepsonseID's is (kinda)\n",
    "# check if the indices of a responseID here are the same after transforming the ResponseID column\n",
    "# if the indices are the same means ResponseID column is succesfully transformed\n",
    "\n",
    "indices = df_country_98[df_country_98[\"ResponseID\"] == '2224kBG72574tbZD3'].index\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esmku\\AppData\\Local\\Temp\\ipykernel_28492\\1528720168.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_country_98['ResponseID'] = df_country_98['ResponseID'].map(response_id_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Changing the responseID\n",
    "\n",
    "# Define the starting point for the new ResponseID\n",
    "starting_id = 146784\n",
    "\n",
    "# Step 1: Get the unique ResponseIDs\n",
    "unique_response_ids = df_country_98['ResponseID'].unique()\n",
    "\n",
    "# Step 2: Create a mapping from old ResponseID to new 'res_' formatted ID\n",
    "response_id_mapping = {old_id: f'res_{i:08d}' for i, old_id in enumerate(unique_response_ids, starting_id)}\n",
    "\n",
    "# Step 3: Replace the original ResponseID with the new mapped IDs\n",
    "df_country_98['ResponseID'] = df_country_98['ResponseID'].map(response_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>ExtendedSessionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>ScenarioOrder</th>\n",
       "      <th>Intervention</th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>AttributeLevel</th>\n",
       "      <th>ScenarioTypeStrict</th>\n",
       "      <th>...</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>res_00146784</td>\n",
       "      <td>737909459_839962439872333.0</td>\n",
       "      <td>8.399624e+14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Old</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>res_00146785</td>\n",
       "      <td>-1700464608_947225429117679.0</td>\n",
       "      <td>9.472254e+14</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>res_00146786</td>\n",
       "      <td>596330438_8731233368451661.0</td>\n",
       "      <td>8.731233e+15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Young</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>res_00146787</td>\n",
       "      <td>-1232628507_1597557389</td>\n",
       "      <td>1.597557e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gender</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>res_00146788</td>\n",
       "      <td>329642844_496228042967179.0</td>\n",
       "      <td>4.962280e+14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pets</td>\n",
       "      <td>Species</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ResponseID              ExtendedSessionID        UserID  ScenarioOrder  \\\n",
       "0  res_00146784    737909459_839962439872333.0  8.399624e+14             12   \n",
       "2  res_00146785  -1700464608_947225429117679.0  9.472254e+14              7   \n",
       "3  res_00146786   596330438_8731233368451661.0  8.731233e+15              6   \n",
       "4  res_00146787         -1232628507_1597557389  1.597557e+09              5   \n",
       "6  res_00146788    329642844_496228042967179.0  4.962280e+14              4   \n",
       "\n",
       "   Intervention  PedPed  Barrier  CrossingSignal AttributeLevel  \\\n",
       "0             0       0        1               0            Old   \n",
       "2             0       0        0               0           More   \n",
       "3             0       1        0               0          Young   \n",
       "4             0       0        1               0         Female   \n",
       "6             0       1        0               0           Pets   \n",
       "\n",
       "  ScenarioTypeStrict  ... LargeMan Criminal MaleExecutive  FemaleExecutive  \\\n",
       "0                Age  ...      0.0      0.0           0.0              0.0   \n",
       "2        Utilitarian  ...      0.0      0.0           0.0              0.0   \n",
       "3                Age  ...      0.0      0.0           0.0              0.0   \n",
       "4             Gender  ...      0.0      0.0           0.0              0.0   \n",
       "6            Species  ...      0.0      0.0           0.0              0.0   \n",
       "\n",
       "   FemaleAthlete  MaleAthlete  FemaleDoctor MaleDoctor  Dog  Cat  \n",
       "0            0.0          0.0           0.0        0.0  0.0  0.0  \n",
       "2            0.0          1.0           0.0        0.0  0.0  0.0  \n",
       "3            0.0          0.0           0.0        0.0  0.0  0.0  \n",
       "4            0.0          0.0           0.0        0.0  0.0  0.0  \n",
       "6            0.0          0.0           0.0        0.0  0.0  2.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResponseID starts with res_00146784\n",
    "df_country_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([0, 884580], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "indices = df_country_98[df_country_98[\"ResponseID\"] == 'res_00146784'].index\n",
    "print(indices)\n",
    "\n",
    "# checking if the indices are the same after transforming the ResponseID column\n",
    "# compared to the indices of the ResponseID before transforming the column (which was 2224kBG72574tbZD3) - they are!\n",
    "# both are [2, 884580]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>ExtendedSessionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>ScenarioOrder</th>\n",
       "      <th>Intervention</th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>AttributeLevel</th>\n",
       "      <th>ScenarioTypeStrict</th>\n",
       "      <th>...</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6999991</th>\n",
       "      <td>res_02596779</td>\n",
       "      <td>1615899623_514570710</td>\n",
       "      <td>5.145707e+08</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Pets</td>\n",
       "      <td>Species</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999995</th>\n",
       "      <td>res_02596780</td>\n",
       "      <td>-1882580163_3293063696269629.0</td>\n",
       "      <td>3.293064e+15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Fat</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999996</th>\n",
       "      <td>res_02596781</td>\n",
       "      <td>-1863857309_1456185995</td>\n",
       "      <td>1.456186e+09</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Young</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999997</th>\n",
       "      <td>res_02596782</td>\n",
       "      <td>1946363904_4846540250658522.0</td>\n",
       "      <td>4.846540e+15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gender</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999999</th>\n",
       "      <td>res_02596783</td>\n",
       "      <td>434100736_1451013350</td>\n",
       "      <td>1.451013e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ResponseID               ExtendedSessionID        UserID  \\\n",
       "6999991  res_02596779            1615899623_514570710  5.145707e+08   \n",
       "6999995  res_02596780  -1882580163_3293063696269629.0  3.293064e+15   \n",
       "6999996  res_02596781          -1863857309_1456185995  1.456186e+09   \n",
       "6999997  res_02596782   1946363904_4846540250658522.0  4.846540e+15   \n",
       "6999999  res_02596783            434100736_1451013350  1.451013e+09   \n",
       "\n",
       "         ScenarioOrder  Intervention  PedPed  Barrier  CrossingSignal  \\\n",
       "6999991              5             1       0        1               0   \n",
       "6999995              2             1       0        1               0   \n",
       "6999996              6             1       1        0               1   \n",
       "6999997              1             1       1        0               1   \n",
       "6999999              1             1       1        0               0   \n",
       "\n",
       "        AttributeLevel ScenarioTypeStrict  ... LargeMan Criminal  \\\n",
       "6999991           Pets            Species  ...      0.0      0.0   \n",
       "6999995            Fat            Fitness  ...      1.0      0.0   \n",
       "6999996          Young                Age  ...      0.0      0.0   \n",
       "6999997         Female             Gender  ...      0.0      0.0   \n",
       "6999999           More        Utilitarian  ...      0.0      0.0   \n",
       "\n",
       "        MaleExecutive  FemaleExecutive  FemaleAthlete  MaleAthlete  \\\n",
       "6999991           0.0              0.0            0.0          0.0   \n",
       "6999995           0.0              0.0            0.0          0.0   \n",
       "6999996           0.0              0.0            0.0          0.0   \n",
       "6999997           0.0              0.0            0.0          0.0   \n",
       "6999999           0.0              0.0            0.0          0.0   \n",
       "\n",
       "         FemaleDoctor MaleDoctor  Dog  Cat  \n",
       "6999991           0.0        0.0  2.0  1.0  \n",
       "6999995           0.0        0.0  0.0  0.0  \n",
       "6999996           0.0        0.0  0.0  0.0  \n",
       "6999997           0.0        0.0  0.0  0.0  \n",
       "6999999           0.0        0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_98.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ResponseID', 'ExtendedSessionID', 'UserID', 'ScenarioOrder',\n",
       "       'Intervention', 'PedPed', 'Barrier', 'CrossingSignal', 'AttributeLevel',\n",
       "       'ScenarioTypeStrict', 'ScenarioType', 'DefaultChoice',\n",
       "       'NonDefaultChoice', 'DefaultChoiceIsOmission', 'NumberOfCharacters',\n",
       "       'DiffNumberOFCharacters', 'Saved', 'Template', 'DescriptionShown',\n",
       "       'LeftHand', 'UserCountry3', 'Man', 'Woman', 'Pregnant', 'Stroller',\n",
       "       'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless', 'LargeWoman',\n",
       "       'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive',\n",
       "       'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor', 'MaleDoctor', 'Dog',\n",
       "       'Cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_98.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already deleting some columns to ensure it fits in memory\n",
    "# deleting the columns that are not necessary for the modelling: ExtendedSessionID, DefaultChoice, NonDefaultChoice, DefaultChoiceIsOmission, Template\n",
    "\n",
    "df_country_clean = df_country_98.drop(columns=['ExtendedSessionID', 'DefaultChoice', 'NonDefaultChoice', 'DefaultChoiceIsOmission', 'Template', 'ScenarioType', 'ScenarioOrder', 'DescriptionShown', 'LeftHand', 'UserCountry3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900000, 31)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_clean.shape # should be 31 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index\n",
    "df_country_clean = df_country_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizing UserID - making all into 0\n",
    "df_country_clean['UserID'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this pre-preprocessed non-western dataset to a csv file\n",
    "\n",
    "df_country_clean.to_csv('non_western_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
