{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "why do I add to it? why not just a completly new dataset? Data leakage can be an argument\n",
    "then just taking 17M (more than the 14384832 needed rows, since need to account for NAN's) rows from the complete_responseid_country dataset! \"\"\"\n",
    "\n",
    "# do the same preprocessing for the complete_responseid_country dataset as for the mme_dataset, so also responseID change and removing random from typestrict and removing columns\n",
    "\n",
    "\"DOING THE SAME AS FOR THE MME_DATASET, BUT THEN FOR ONLY COUNTRIES *NOT* IN THE WESTERN LIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a dataframe with only the UserCountry3 column from the 71M SharedResponses.csv file\n",
    "reader_country = pd.read_csv('SharedResponses.csv', usecols=['UserCountry3', 'ResponseID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries considered 'Western' from the UN\n",
    "\n",
    "countries = [\"AND\",  \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DNK\", \"DEU\", \"ESP\", \"FIN\",  \"FRA\",  \"GBR\",  \"GRC\",  \"IRL\", \n",
    "            \"ISR\", \"ISL\", \"ITA\", \"LIE\", \"LUX\", \"MLT\", \"MCO\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SMR\", \"SWE\", \"TUR\", \"USA\" ]\n",
    "\n",
    "# Andorra, Australia, Austria, Belgium, Canada, Switzerland, Denmark, Germany, Spain, Finland, France, United Kingdom, Greece, Ireland, \n",
    "# Israel, Iceland, Italy, Liechtenstein, Luxembourg, Malta, Monaco, Netherlands, Norway, New Zealand, Portugal, San Marino, Sweden, Turkey, USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where UserCountryID is NOT the list of countries that are considered Western\n",
    "\n",
    "non_western = reader_country[~reader_country['UserCountry3'].isin(countries)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21837017, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_western.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep complete scenarios of 2\n",
    "\n",
    "# Count how many times each ResponseID appears\n",
    "response_counts = non_western['ResponseID'].value_counts()\n",
    "\n",
    "# Get the ResponseIDs that appear exactly twice\n",
    "ids_to_keep = response_counts[response_counts == 2].index\n",
    "\n",
    "# Filter the rows where ResponseID is in the list of ids_to_keep\n",
    "complete_responseid_country = non_western[non_western['ResponseID'].isin(ids_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(complete_responseid_country.value_counts().min())\n",
    "print(complete_responseid_country.value_counts().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21084418, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_responseid_country.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# checking the NAN's in UserCountry3 in the complete_responseid_country dataset and removing them\n",
    "print(complete_responseid_country['UserCountry3'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(20439550, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing the NAN's\n",
    "complete_responseid_country = complete_responseid_country.dropna(subset=['UserCountry3'])\n",
    "print(complete_responseid_country['UserCountry3'].isna().sum())\n",
    "print(complete_responseid_country.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on 'ResponseID' and keep the first occurrence\n",
    "complete_responseid_country = complete_responseid_country.drop_duplicates(subset=['ResponseID'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_subset = complete_responseid_country.tail(8500000)  # want 17M rows (with accounting for NAn's and deleting rows with 'random' (around 10%)), so 17M / 2 = 8.5M ResponseID's necessary\n",
    "\n",
    "# taking the last 8,5 Million, as I assume the data in the set is random enough to not have a bias (like country) in the first 8,5M rows\n",
    "# furthermore, this ensures new data, and not the same responses that were in the MME_total dataset (idk if this is important!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseID      8500000\n",
       "UserCountry3        202\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if they are all unique\n",
    "\n",
    "country_subset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500000\n"
     ]
    }
   ],
   "source": [
    "# transform to a list to feed to the for loop that will extract all the corresponding columns from the SharedResponses.csv file\n",
    "\n",
    "country_sub_list = country_subset['ResponseID'].tolist()\n",
    "# check if it went okay\n",
    "print(len(country_sub_list))     # should be 8500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_country = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Finished processing chunk 1\n",
      "Processing chunk 2\n",
      "Finished processing chunk 2\n",
      "Processing chunk 3\n",
      "Finished processing chunk 3\n",
      "Processing chunk 4\n",
      "Finished processing chunk 4\n",
      "Processing chunk 5\n",
      "Finished processing chunk 5\n",
      "Processing chunk 6\n",
      "Finished processing chunk 6\n",
      "Processing chunk 7\n",
      "Finished processing chunk 7\n",
      "Processing chunk 8\n",
      "Finished processing chunk 8\n",
      "Processing chunk 9\n",
      "Finished processing chunk 9\n",
      "Processing chunk 10\n",
      "Finished processing chunk 10\n",
      "Processing chunk 11\n",
      "Finished processing chunk 11\n",
      "Processing chunk 12\n",
      "Finished processing chunk 12\n",
      "Processing chunk 13\n",
      "Finished processing chunk 13\n",
      "Processing chunk 14\n",
      "Finished processing chunk 14\n",
      "Processing chunk 15\n",
      "Finished processing chunk 15\n",
      "Processing chunk 16\n",
      "Finished processing chunk 16\n",
      "Processing chunk 17\n",
      "Finished processing chunk 17\n",
      "Processing chunk 18\n",
      "Finished processing chunk 18\n",
      "Processing chunk 19\n",
      "Finished processing chunk 19\n",
      "Processing chunk 20\n",
      "Finished processing chunk 20\n",
      "Processing chunk 21\n",
      "Finished processing chunk 21\n",
      "Processing chunk 22\n",
      "Finished processing chunk 22\n",
      "Processing chunk 23\n",
      "Finished processing chunk 23\n",
      "Processing chunk 24\n",
      "Finished processing chunk 24\n",
      "Processing chunk 25\n",
      "Finished processing chunk 25\n",
      "Processing chunk 26\n",
      "Finished processing chunk 26\n",
      "Processing chunk 27\n",
      "Finished processing chunk 27\n",
      "Processing chunk 28\n",
      "Finished processing chunk 28\n",
      "Processing chunk 29\n",
      "Finished processing chunk 29\n",
      "Processing chunk 30\n",
      "Finished processing chunk 30\n",
      "Processing chunk 31\n",
      "Finished processing chunk 31\n",
      "Processing chunk 32\n",
      "Finished processing chunk 32\n",
      "Processing chunk 33\n",
      "Finished processing chunk 33\n",
      "Processing chunk 34\n",
      "Finished processing chunk 34\n",
      "Processing chunk 35\n",
      "Finished processing chunk 35\n",
      "Processing chunk 36\n",
      "Finished processing chunk 36\n",
      "Processing chunk 37\n",
      "Finished processing chunk 37\n",
      "Processing chunk 38\n",
      "Finished processing chunk 38\n",
      "Processing chunk 39\n",
      "Finished processing chunk 39\n",
      "Processing chunk 40\n",
      "Finished processing chunk 40\n",
      "Processing chunk 41\n",
      "Finished processing chunk 41\n",
      "Processing chunk 42\n",
      "Finished processing chunk 42\n",
      "Processing chunk 43\n",
      "Finished processing chunk 43\n",
      "Processing chunk 44\n",
      "Finished processing chunk 44\n",
      "Processing chunk 45\n",
      "Finished processing chunk 45\n",
      "Processing chunk 46\n",
      "Finished processing chunk 46\n",
      "Processing chunk 47\n",
      "Finished processing chunk 47\n",
      "Processing chunk 48\n",
      "Finished processing chunk 48\n",
      "Processing chunk 49\n",
      "Finished processing chunk 49\n",
      "Processing chunk 50\n",
      "Finished processing chunk 50\n",
      "Processing chunk 51\n",
      "Finished processing chunk 51\n",
      "Processing chunk 52\n",
      "Finished processing chunk 52\n",
      "Processing chunk 53\n",
      "Finished processing chunk 53\n",
      "Processing chunk 54\n",
      "Finished processing chunk 54\n",
      "Processing chunk 55\n",
      "Finished processing chunk 55\n",
      "Processing chunk 56\n",
      "Finished processing chunk 56\n",
      "Processing chunk 57\n",
      "Finished processing chunk 57\n",
      "Processing chunk 58\n",
      "Finished processing chunk 58\n",
      "Processing chunk 59\n",
      "Finished processing chunk 59\n",
      "Processing chunk 60\n",
      "Finished processing chunk 60\n",
      "Processing chunk 61\n",
      "Finished processing chunk 61\n",
      "Processing chunk 62\n",
      "Finished processing chunk 62\n",
      "Processing chunk 63\n",
      "Finished processing chunk 63\n",
      "Processing chunk 64\n",
      "Finished processing chunk 64\n",
      "Processing chunk 65\n",
      "Finished processing chunk 65\n",
      "Processing chunk 66\n",
      "Finished processing chunk 66\n",
      "Processing chunk 67\n",
      "Finished processing chunk 67\n",
      "Processing chunk 68\n",
      "Finished processing chunk 68\n",
      "Processing chunk 69\n",
      "Finished processing chunk 69\n",
      "Processing chunk 70\n",
      "Finished processing chunk 70\n",
      "Processing chunk 71\n",
      "Finished processing chunk 71\n",
      "Processing chunk 72\n",
      "Finished processing chunk 72\n",
      "Processing chunk 73\n",
      "Finished processing chunk 73\n",
      "Processing chunk 74\n",
      "Finished processing chunk 74\n",
      "Processing chunk 75\n",
      "Finished processing chunk 75\n",
      "Processing chunk 76\n",
      "Finished processing chunk 76\n",
      "Processing chunk 77\n",
      "Finished processing chunk 77\n",
      "Processing chunk 78\n",
      "Finished processing chunk 78\n",
      "Processing chunk 79\n",
      "Finished processing chunk 79\n",
      "Processing chunk 80\n",
      "Finished processing chunk 80\n",
      "Processing chunk 81\n",
      "Finished processing chunk 81\n",
      "Processing chunk 82\n",
      "Finished processing chunk 82\n",
      "Processing chunk 83\n",
      "Finished processing chunk 83\n",
      "Processing chunk 84\n",
      "Finished processing chunk 84\n",
      "Processing chunk 85\n",
      "Finished processing chunk 85\n",
      "Processing chunk 86\n",
      "Finished processing chunk 86\n",
      "Processing chunk 87\n",
      "Finished processing chunk 87\n",
      "Processing chunk 88\n",
      "Finished processing chunk 88\n",
      "Processing chunk 89\n",
      "Finished processing chunk 89\n",
      "Processing chunk 90\n",
      "Finished processing chunk 90\n",
      "Processing chunk 91\n",
      "Finished processing chunk 91\n",
      "Processing chunk 92\n",
      "Finished processing chunk 92\n",
      "Processing chunk 93\n",
      "Finished processing chunk 93\n",
      "Processing chunk 94\n",
      "Finished processing chunk 94\n",
      "Processing chunk 95\n",
      "Finished processing chunk 95\n",
      "Processing chunk 96\n",
      "Finished processing chunk 96\n",
      "Processing chunk 97\n",
      "Finished processing chunk 97\n",
      "Processing chunk 98\n",
      "Finished processing chunk 98\n",
      "Processing chunk 99\n",
      "Finished processing chunk 99\n",
      "Processing chunk 100\n",
      "Finished processing chunk 100\n",
      "Processing chunk 101\n",
      "Finished processing chunk 101\n",
      "Processing chunk 102\n",
      "Finished processing chunk 102\n",
      "Processing chunk 103\n",
      "Finished processing chunk 103\n",
      "Processing chunk 104\n",
      "Finished processing chunk 104\n",
      "Processing chunk 105\n",
      "Finished processing chunk 105\n",
      "Processing chunk 106\n",
      "Finished processing chunk 106\n",
      "Processing chunk 107\n",
      "Finished processing chunk 107\n",
      "Processing chunk 108\n",
      "Finished processing chunk 108\n",
      "Processing chunk 109\n",
      "Finished processing chunk 109\n",
      "Processing chunk 110\n",
      "Finished processing chunk 110\n",
      "Processing chunk 111\n",
      "Finished processing chunk 111\n",
      "Processing chunk 112\n",
      "Finished processing chunk 112\n",
      "Processing chunk 113\n",
      "Finished processing chunk 113\n",
      "Processing chunk 114\n",
      "Finished processing chunk 114\n",
      "Processing chunk 115\n",
      "Finished processing chunk 115\n",
      "Processing chunk 116\n",
      "Finished processing chunk 116\n",
      "Processing chunk 117\n",
      "Finished processing chunk 117\n",
      "Processing chunk 118\n",
      "Finished processing chunk 118\n",
      "Processing chunk 119\n",
      "Finished processing chunk 119\n",
      "Processing chunk 120\n",
      "Finished processing chunk 120\n",
      "Processing chunk 121\n",
      "Finished processing chunk 121\n",
      "Processing chunk 122\n",
      "Finished processing chunk 122\n",
      "Processing chunk 123\n",
      "Finished processing chunk 123\n",
      "Processing chunk 124\n",
      "Finished processing chunk 124\n",
      "Processing chunk 125\n",
      "Finished processing chunk 125\n",
      "Processing chunk 126\n",
      "Finished processing chunk 126\n",
      "Processing chunk 127\n",
      "Finished processing chunk 127\n",
      "Processing chunk 128\n",
      "Finished processing chunk 128\n",
      "Processing chunk 129\n",
      "Finished processing chunk 129\n",
      "Processing chunk 130\n",
      "Finished processing chunk 130\n",
      "Processing chunk 131\n",
      "Finished processing chunk 131\n",
      "Processing chunk 132\n",
      "Finished processing chunk 132\n",
      "Processing chunk 133\n",
      "Finished processing chunk 133\n",
      "Processing chunk 134\n",
      "Finished processing chunk 134\n",
      "Processing chunk 135\n",
      "Finished processing chunk 135\n",
      "Processing chunk 136\n",
      "Finished processing chunk 136\n",
      "Processing chunk 137\n",
      "Finished processing chunk 137\n",
      "Processing chunk 138\n",
      "Finished processing chunk 138\n",
      "Processing chunk 139\n",
      "Finished processing chunk 139\n",
      "Processing chunk 140\n",
      "Finished processing chunk 140\n",
      "Processing chunk 141\n",
      "Finished processing chunk 141\n",
      "All chunks have been processed and combined.\n"
     ]
    }
   ],
   "source": [
    "# reading SharedResponses to extract the rows with ResponseID's in country_sub_list\n",
    "# this will result in a dataframe 'subset' that contains around 17 million rows (8,5 * 2)\n",
    "\n",
    "chunk_size = 500_000\n",
    "reader = pd.read_csv('SharedResponses.csv', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "for i, chunk in enumerate(reader):\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}\")\n",
    "\n",
    "    # Filter rows where ResponseID is in reader_subset\n",
    "    subset_chunk = chunk[chunk['ResponseID'].isin(country_sub_list)]\n",
    "\n",
    "    # Append filtered chunk to empty df\n",
    "    subset_country = pd.concat([subset_country, subset_chunk], ignore_index=True)\n",
    "\n",
    "    print(f\"Finished processing chunk {i+1}\")\n",
    "\n",
    "print(\"All chunks have been processed and combined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17000000, 41)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_country.shape  # 17M rows, 41 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserCountry3\n",
      "False    17000000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking to see if the UserCountry3 column only contains countries not in the Western list\n",
    "print(subset_country['UserCountry3'].isin(countries).value_counts())\n",
    "\n",
    "# should be False for all rows (17M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_country_csv = subset_country.to_csv('subset_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esmku\\AppData\\Local\\Temp\\ipykernel_24844\\2704394548.py:2: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_country = pd.read_csv('subset_country.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_country = pd.read_csv('subset_country.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_country.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17000000, 41)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScenarioTypeStrict\n",
       "Utilitarian      3051804\n",
       "Species          2999718\n",
       "Fitness          2998592\n",
       "Age              2996212\n",
       "Gender           2995012\n",
       "Random           1495412\n",
       "Social Status     463250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country['ScenarioTypeStrict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows with 'random' in the ScenarioTypeStrict column\n",
    "\n",
    "df_country = df_country[df_country['ScenarioTypeStrict'] != 'Random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15504588, 41)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.shape # should be 17000000 - 1495412 = 15504588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScenarioTypeStrict\n",
       "Utilitarian      3051804\n",
       "Species          2999718\n",
       "Fitness          2998592\n",
       "Age              2996212\n",
       "Gender           2995012\n",
       "Social Status     463250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country['ScenarioTypeStrict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseID                       0\n",
       "ExtendedSessionID                0\n",
       "UserID                         822\n",
       "ScenarioOrder                    0\n",
       "Intervention                     0\n",
       "PedPed                           0\n",
       "Barrier                          0\n",
       "CrossingSignal                   0\n",
       "AttributeLevel                   0\n",
       "ScenarioTypeStrict               0\n",
       "ScenarioType                     2\n",
       "DefaultChoice               492908\n",
       "NonDefaultChoice            492908\n",
       "DefaultChoiceIsOmission     492908\n",
       "NumberOfCharacters               2\n",
       "DiffNumberOFCharacters           2\n",
       "Saved                            0\n",
       "Template                   1855572\n",
       "DescriptionShown           1855572\n",
       "LeftHand                   1855572\n",
       "UserCountry3                     0\n",
       "Man                              1\n",
       "Woman                            2\n",
       "Pregnant                         2\n",
       "Stroller                         2\n",
       "OldMan                           2\n",
       "OldWoman                         2\n",
       "Boy                              2\n",
       "Girl                             2\n",
       "Homeless                         2\n",
       "LargeWoman                       2\n",
       "LargeMan                         2\n",
       "Criminal                         2\n",
       "MaleExecutive                    2\n",
       "FemaleExecutive                  2\n",
       "FemaleAthlete                    2\n",
       "MaleAthlete                      2\n",
       "FemaleDoctor                     2\n",
       "MaleDoctor                       2\n",
       "Dog                              2\n",
       "Cat                              2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows with NaN's in the UserID column\n",
    "\n",
    "df_country = df_country.dropna(subset=['UserID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15503766, 41)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.shape # should be 15504588 - 822 = 15503766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14384832.0\n",
      "1118934.0\n"
     ]
    }
   ],
   "source": [
    "# rows that are needed to make dataset 2% llm and 98% human\n",
    "total_rows_dataset = 293568 / 2 * 98\n",
    "print(total_rows_dataset)    \n",
    "rows_to_delete = df_country.shape[0] - total_rows_dataset\n",
    "print(rows_to_delete) \n",
    "\n",
    "# need to delete 1118934 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7751883\n"
     ]
    }
   ],
   "source": [
    "# randomly delete 1118934 / 2 = 559467 unique UserIDs (is 1118934 rows), to ensure 2% of the dataset is LLMs and 98% humans\n",
    "\n",
    "# Getting unique UserIDs\n",
    "Response_unique = df_country['ResponseID'].unique()\n",
    "print(len(Response_unique))  # should be 15503766/2 = 7751883\n",
    "\n",
    "# Selecting 559467 UserIDs from the unique set\n",
    "Response_delete = pd.Series(Response_unique).sample(n=559467, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_98 = df_country[~df_country['ResponseID'].isin(Response_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14384832, 41)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of df_filtered\n",
    "\n",
    "df_country_98.shape   # should be 15503310 - 1118934 = 14384832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esmku\\AppData\\Local\\Temp\\ipykernel_24844\\2329753221.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_country_98['ResponseID'] = [f'res_{starting_id + i // 2:08d}' for i in range(num_rows)]\n"
     ]
    }
   ],
   "source": [
    "# Changing the responseID\n",
    "\n",
    "# Define the starting point for the new ResponseID\n",
    "starting_id = 146784        # cause this is where llm rows ended\n",
    "\n",
    "# Get the total number of rows\n",
    "num_rows = len(df_country_98)\n",
    "\n",
    "# Create a new column where every two rows have the same ResponseID\n",
    "df_country_98['ResponseID'] = [f'res_{starting_id + i // 2:08d}' for i in range(num_rows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>ExtendedSessionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>ScenarioOrder</th>\n",
       "      <th>Intervention</th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>AttributeLevel</th>\n",
       "      <th>ScenarioTypeStrict</th>\n",
       "      <th>...</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>res_00146784</td>\n",
       "      <td>477341325_9658440103616898.0</td>\n",
       "      <td>9.658440e+15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hoomans</td>\n",
       "      <td>Species</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>res_00146784</td>\n",
       "      <td>-1933553294_3514601197273468.0</td>\n",
       "      <td>3.514601e+15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pets</td>\n",
       "      <td>Species</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>res_00146785</td>\n",
       "      <td>-1906528630_1615619368</td>\n",
       "      <td>1.615619e+09</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fit</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>res_00146785</td>\n",
       "      <td>1900197736_3778157516.0</td>\n",
       "      <td>3.778158e+09</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Less</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>res_00146786</td>\n",
       "      <td>1256110879_1407997320198901.0</td>\n",
       "      <td>1.407997e+15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>Gender</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ResponseID               ExtendedSessionID        UserID  ScenarioOrder  \\\n",
       "1  res_00146784    477341325_9658440103616898.0  9.658440e+15             10   \n",
       "4  res_00146784  -1933553294_3514601197273468.0  3.514601e+15             11   \n",
       "5  res_00146785          -1906528630_1615619368  1.615619e+09              7   \n",
       "6  res_00146785         1900197736_3778157516.0  3.778158e+09             10   \n",
       "7  res_00146786   1256110879_1407997320198901.0  1.407997e+15             10   \n",
       "\n",
       "   Intervention  PedPed  Barrier  CrossingSignal AttributeLevel  \\\n",
       "1             0       0        0               0        Hoomans   \n",
       "4             0       1        0               0           Pets   \n",
       "5             0       0        0               0            Fit   \n",
       "6             0       1        0               0           Less   \n",
       "7             0       0        0               2           Male   \n",
       "\n",
       "  ScenarioTypeStrict  ... LargeMan Criminal MaleExecutive  FemaleExecutive  \\\n",
       "1            Species  ...      0.0      0.0           1.0              1.0   \n",
       "4            Species  ...      0.0      0.0           0.0              0.0   \n",
       "5            Fitness  ...      0.0      0.0           0.0              0.0   \n",
       "6        Utilitarian  ...      1.0      0.0           0.0              0.0   \n",
       "7             Gender  ...      0.0      0.0           0.0              0.0   \n",
       "\n",
       "   FemaleAthlete  MaleAthlete  FemaleDoctor MaleDoctor  Dog  Cat  \n",
       "1            0.0          0.0           0.0        0.0  0.0  0.0  \n",
       "4            0.0          0.0           0.0        0.0  3.0  0.0  \n",
       "5            2.0          1.0           0.0        0.0  0.0  0.0  \n",
       "6            0.0          0.0           0.0        0.0  0.0  0.0  \n",
       "7            0.0          0.0           0.0        0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResponseID starts with res_00146784\n",
    "df_country_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>ExtendedSessionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>ScenarioOrder</th>\n",
       "      <th>Intervention</th>\n",
       "      <th>PedPed</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>AttributeLevel</th>\n",
       "      <th>ScenarioTypeStrict</th>\n",
       "      <th>...</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16999994</th>\n",
       "      <td>res_07339197</td>\n",
       "      <td>1135690765_653858658</td>\n",
       "      <td>6.538587e+08</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fat</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999995</th>\n",
       "      <td>res_07339198</td>\n",
       "      <td>625350849_8584219519940754.0</td>\n",
       "      <td>8.584220e+15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Pets</td>\n",
       "      <td>Species</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999997</th>\n",
       "      <td>res_07339198</td>\n",
       "      <td>434100736_1451013350</td>\n",
       "      <td>1.451013e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999998</th>\n",
       "      <td>res_07339199</td>\n",
       "      <td>-1815706184_742769765189808.0</td>\n",
       "      <td>7.427698e+14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Young</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999999</th>\n",
       "      <td>res_07339199</td>\n",
       "      <td>-1475685997_4499486314855814.0</td>\n",
       "      <td>4.499486e+15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fit</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ResponseID               ExtendedSessionID        UserID  \\\n",
       "16999994  res_07339197            1135690765_653858658  6.538587e+08   \n",
       "16999995  res_07339198    625350849_8584219519940754.0  8.584220e+15   \n",
       "16999997  res_07339198            434100736_1451013350  1.451013e+09   \n",
       "16999998  res_07339199   -1815706184_742769765189808.0  7.427698e+14   \n",
       "16999999  res_07339199  -1475685997_4499486314855814.0  4.499486e+15   \n",
       "\n",
       "          ScenarioOrder  Intervention  PedPed  Barrier  CrossingSignal  \\\n",
       "16999994             13             1       0        0               0   \n",
       "16999995              6             1       1        0               1   \n",
       "16999997              1             1       1        0               0   \n",
       "16999998              5             1       1        0               0   \n",
       "16999999              4             1       1        0               2   \n",
       "\n",
       "         AttributeLevel ScenarioTypeStrict  ... LargeMan Criminal  \\\n",
       "16999994            Fat            Fitness  ...      2.0      0.0   \n",
       "16999995           Pets            Species  ...      0.0      0.0   \n",
       "16999997           More        Utilitarian  ...      0.0      0.0   \n",
       "16999998          Young                Age  ...      0.0      0.0   \n",
       "16999999            Fit            Fitness  ...      0.0      0.0   \n",
       "\n",
       "         MaleExecutive  FemaleExecutive  FemaleAthlete  MaleAthlete  \\\n",
       "16999994           0.0              0.0            0.0          0.0   \n",
       "16999995           0.0              0.0            0.0          0.0   \n",
       "16999997           0.0              0.0            0.0          0.0   \n",
       "16999998           0.0              0.0            0.0          0.0   \n",
       "16999999           0.0              0.0            1.0          1.0   \n",
       "\n",
       "          FemaleDoctor MaleDoctor  Dog  Cat  \n",
       "16999994           0.0        0.0  0.0  0.0  \n",
       "16999995           0.0        0.0  2.0  2.0  \n",
       "16999997           0.0        0.0  0.0  0.0  \n",
       "16999998           0.0        0.0  0.0  0.0  \n",
       "16999999           0.0        0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResponseID ends with res_00146784 + (14384832/2) = res_07339200\n",
    "df_country_98.tail()\n",
    "\n",
    "# somehow the last responseID is res_07339199, but it is correct I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ResponseID', 'ExtendedSessionID', 'UserID', 'ScenarioOrder',\n",
       "       'Intervention', 'PedPed', 'Barrier', 'CrossingSignal', 'AttributeLevel',\n",
       "       'ScenarioTypeStrict', 'ScenarioType', 'DefaultChoice',\n",
       "       'NonDefaultChoice', 'DefaultChoiceIsOmission', 'NumberOfCharacters',\n",
       "       'DiffNumberOFCharacters', 'Saved', 'Template', 'DescriptionShown',\n",
       "       'LeftHand', 'UserCountry3', 'Man', 'Woman', 'Pregnant', 'Stroller',\n",
       "       'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless', 'LargeWoman',\n",
       "       'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive',\n",
       "       'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor', 'MaleDoctor', 'Dog',\n",
       "       'Cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_98.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already deleting some columns to ensure it fits in memory\n",
    "# deleting the columns that are not necessary for the modelling: ExtendedSessionID, DefaultChoice, NonDefaultChoice, DefaultChoiceIsOmission, Template\n",
    "\n",
    "df_country_clean = df_country_98.drop(columns=['ExtendedSessionID', 'DefaultChoice', 'NonDefaultChoice', 'DefaultChoiceIsOmission', 'Template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14384832, 36)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_clean.shape # should be 14367864 rows and 41-5 = 36 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this pre-preprocessed non-western dataset to a csv file\n",
    "\n",
    "df_country_clean.to_csv('non_western_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
